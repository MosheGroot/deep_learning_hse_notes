\documentclass{article}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage[left=3cm,right=3cm,
    top=3cm,bottom=3cm,bindingoffset=0cm]{geometry}

\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}

\usepackage{setspace}
\usepackage{indentfirst}
\usepackage{textcomp}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{mathrsfs}


\title{Введение в машинное обучение}
\author{Национальный исследовательский университет "Высшая школа экономики" \and Yandex School of Data Analysis\\\\
Неофициальный конспект по курсу.}

\begin{document}
\maketitle

\section{Формальная постановка задачи машинного обучения}

Машинное обучение последнее время применяется активно в прикладных задачах, где накоплены большие объемы информации и требуется решить такие задачи, как \textit{предсказание, автоматизация принятий решений, классификация}, то есть такие задачи, которые ранее считались прерогативой человека.

Для решения подобных задач требуется:

\begin{enumerate}
\item Накопление большого количества данных;
\item Составление предсказательных моделей.
\end{enumerate}

Именно вторым пунктом мы и будем заниматься в данном курсе.

\subsection{Задача обучения по прецедентам}

По сути дела, большая часть машинного обучения, это наука о том, как решать задачу \textit{восстановления функции по точкам}.
\\

Пусть у нас имеются данные, это множество объектов --- $X$.

Также мы имеем множество ответов (размерность зависит от задачи) --- $Y$.

Тогда наша предсказательная модель по своей сути -- это отображение вида:
\\

$\qquad y : X \rightarrow Y$ --- неизвестная зависимость (\textbf{target function})
\\

Именно ее мы хотим найти, по крайней мере зная, что она промерена в конечном множестве точек, которое называется \textit{"обучающей выборкой"}:
\\

$\qquad \{x_1, \ldots, x_l\} \subset X$ --- обучающая выборка (\textbf{training sample})

$\qquad y_i = y(x_i),\quad i = 1, \ldots,l$ --- известные ответы.
\\

То есть, грубо говоря, имеется $l$ штук пар "объект-ответ", и нам хочется по этой информации восстановить эту зависимость, или построить функцию, которая будет аппроксимировать эту зависимость:
\\

$\qquad a: X \rightarrow Y$ --- алгоритм, решающий (аппроксимирующий) функцию (\textbf{decision function}),
приближающую $y$ на всем множестве $X$.
\\

\newpage
Таким образом, весь курс машинного обучения --- это конкретизация:

\begin{itemize}
\item как задаются объекты и какими могут быть объекты;
\item как строить функцию $a$;
\item в каком смысле $a$ должен приближать $y$.
\end{itemize}

\subsection{Как задаются объекты. Признаковое описание.}

Самый распространенный способ задать описание объекта --- признаковое описание.
\\

Чисто формально, это функции, которые объектам ставят в соответствие какие-то значения (как правило числовые):
\\

$\qquad f_j: X \rightarrow D_j,\quad j = 1, \ldots, n$
--- признаки объектов (\textbf{features})
\\

На содержательном уровне это какие-то способы измерения над объектами. В зависимости от того, что это за измерения, можно выделить различные типы признаков:

\begin{itemize}
\item $D_j = \{0, 1\}$ --- бинарный признак $f_j$;

\item $|D_j| < \infty$ --- номинальный признак $f_j$;

\item $|D_j| < \infty$,   $D_j$ упорядочено ---порядковый признак $f_j$;

\item $D_j = \mathbb{R}$ --- количественный признак $f_j$.
\end{itemize}

Для некоторого объекта $x$ мы можем составить вектор с его описанием:
\\

$\qquad$ Вектор $(f_1(x), \ldots, f_n(x))$ --- \textit{признаковое описание} объекта $x$.
\\

Важно, что количество различных значений для бинарного, номинального и порядкового \textbf{ограничено}. Для количественного признака, очевидно, это не справедливо.

Так же можно отметить, что часто для описания объекта используется \textit{комбинация} различных типов признаков.
\\

Главный объект, который далее будет применяться в решении нашей задачи, будет \textbf{матрица "объекты-признаки" (feature data)} как представление нашей обучающей выборки:

$$F = ||f_j(x_i)||_{l \times n}=
\begin{pmatrix}
f_1(x_1) & \ldots & f_n(x_1)\\
\ldots & \ldots & \ldots\\
f_1(x_l) & \ldots & f_n(x_l)\\
\end{pmatrix}$$

Строкам в этой матрице соответствуют признаковые описания соответствующего объекта (напомним, их $l$ штук), а столбцам соответствуют признаки (конечное количество, $n$ штук). Более того, каждой строке соответствует некоторый правильный ответ (очевидно, этого нет в матрице, но надо помнить, зачем она нам вообще нужна).

\subsection{Как задаются ответы. Типы задач}
\subsubsection{Задачи классификации (classification)}

\begin{itemize}
\item $Y = \{-1, +1\}$ --- классификация на 2 класса.
\item $Y = \{1, \ldots, M\} $ --- на $M$ непересекающихся классов.
\item $Y = \{0, 1\}^M $ --- на $M$ классов, которые могут пересекаться.
\end{itemize}

Классификация на 2 класса соответствует задачам, когда требуется принять одно из двух решений ("собака или кошка").

Второй вариант с $M$ непересекающимися классами соответствует задачам с множественным выбором, например, оптическое распознавание символов рукописного текста.

Третий вариант с $M$ пересекающимися классами подойдет, например, для медицинских задач --- один больной имеет множество заболеваний.

\subsubsection{Задачи восстановления регрессии (regression)}

\begin{itemize}
\item $Y = \mathbb{R}$ или $Y = \mathbb{R}^m$
\end{itemize}

Соответствует задачам, в которых ответы являются действительными числами. К этому классу задач принадлежат огромное количество задач прогнозирования, которые решаются в различных экономических, промышленных приложениях...

\subsubsection{Задачи ранжирования (ranking, learning to rank)}

\begin{itemize}
\item $Y$ -- конечное упорядоченное множество.
\end{itemize}

Например, ранжирование запросов для поисковых сервисов.

\subsection{Предсказательная модель}

После постановки задачи мы двинемся далее, к вопросу о задании предсказательной модели. 
\\

Обычно \textbf{предсказательная модель (predictive model)} подбирается из некоторого семейства параметрических функций:

$$ A = \{a(x) = g(x, \theta)\ | \ \theta \in \Theta\},$$

где $g : X \times \Theta \rightarrow Y$ --- фиксированная функция,

$\Theta $ --- множество допустимых значений параметра $\theta$,

$x$ -- рассматриваемый объект.
\\

Само семейство параметрических функций заранее изобретается, подбирается экспертами такое, что в нем нашлась бы функция, которая хорошо аппроксимирует нашу неизвестную зависимость. Обычно это некоторое семейство $g(x, \theta)$, где $\theta$ -- искомый вектор параметров нашей модели.

\subsubsection{Линейная модель.}

Самый простой и эффективный пример предсказательной модели -- линейная модель.
\\

\textit{Линейная модель} с вектором параметров $\theta = (\theta_1, \ldots, \theta_n), \Theta = \mathbb{R}^n$:
\\

$\qquad g(x, \theta) = \sum\limits_{j = 1}^n \theta_j \cdot f_j(x)$ --- для регрессии и ранжирования, $Y = \mathbb{R}$.
\\

$\qquad g(x, \theta) = sign\sum\limits_{j = 1}^n \theta_j \cdot f_j(x)$ --- для классификации, $Y = \{-1, +1\}$.
\\
 
То есть для регрессии и ранжирования -- это взвешенная сумма всех признаков (весами в данном случае являются значения вектора $\theta$).

Для классификации нам просто достаточно знать, принадлежит ли объект к данному классу или нет (отсюда и использование $sign$). То есть фактически, для задач классификации модель строит разделяющую гиперплоскость в $n-$мерном пространстве, по одну сторону которой находятся объекты одного класса, а по другую --- другого.
\\

И конечно возникает вопрос, почему сумма всех признаков взятых с некоторыми коэффициентами вообще является хорошей моделью для восстанавливаемой зависимости. Такой вопрос в целом возникает каждый раз при решении прикладных задач.

\subsection{Этапы обучения и применения модели}

Важно помнить, что для любой поставленной задачи машинного обучения, у нас всегда выделяются два этапа.
\\

\begin{itemize}

\item\textbf{Этап обучения (train)}:

Кратко: по выборке строим алгоритм-функцию, которая будет предсказывать нам какие-то значения на новых объектах.

Подробно: \textit{Метод обучения (learning algorithm)} $\mu : (X \times Y)^l \rightarrow A$ по выборке $X^l = (x_i, y_i)_{i = 1}^l$ строит алгоритм $a = \mu(X^l)$:

$$\boxed{
\begin{pmatrix}
f_1(x_1) & \ldots & f_n(x_1)\\
\ldots & \ldots & \ldots\\
f_1(x_l) & \ldots & f_n(x_l)\\
\end{pmatrix}
\xrightarrow{\ y\ }
\begin{pmatrix}
y_1\\
\ldots\\
y_l\\
\end{pmatrix}
}\xrightarrow{\ \mu\ } a$$

\item \textbf{Этап применения (test)}:

Кратко: прогон алгоритма по новой выборке объектов. 

Подробно: алгоритм $a$ для новых объектов $x_1^{'}, \ldots, x_k^{'}$ выдает ответы $a(x_i^{'})$:

$$\begin{pmatrix}
f_1(x_1^{'}) & \ldots & f_n(x_1^{'})\\
\ldots & \ldots & \ldots\\
f_1(x_k^{'}) & \ldots & f_n(x_k^{'})\\
\end{pmatrix}
\xrightarrow{\ a\ }
\begin{pmatrix}
a(x_1^{'})\\
\ldots\\
a(x_k^{'})\\
\end{pmatrix}$$
\end{itemize}

\quad

\textbf{Возникает вопрос:} а каким образом мы будем выбирать из предсказательной модели тот алгоритм, который будет нас удовлетворять? Один из способов это сделать --- \textbf{свести задачу к задаче оптимизации}. То есть выбрать такой алгоритм, который на большинстве объектов данной выборки будет точно или достаточно точно выдавать правильные ответы, но чтобы это сделать, нужно разобраться с тем, \textbf{как мы будем определять качество ответа для конкретного объекта}.

\subsection{Функционалы качества}

$\mathscr{L}(a, x)$ --- функция потерь (\textbf{loss function)} --- величина ошибки алгоритма $a \in A$ на объекте $x \in X$.
\\

\textbf{Функции потерь для задач классификации:}
\begin{itemize}
\item $\mathscr{L}(a, x) = [a(x)\not=y(x)]$ --- индикатор ошибки;
\end{itemize}

\quad

\textbf{Функции потерь для задач регрессии:}
\begin{itemize}
\item $\mathscr{L}(a, x) = |a(x) - y(x)|$ --- абсолютное значение ошибки;
\item $\mathscr{L}(a, x) = (a(x) - y(x))^2$ --- квадратичная ошибка.
\end{itemize}

\quad

\textit{Эмпирический риск} --- функционал качества алгоритма $a$ на $X^l$:

$$Q(a, X^l) = \frac{1}{l}\sum\limits_{i = 1}^l \mathscr{L}(a, x_i)$$

И этот функционал качества уже можно минимизировать в задаче оптимизации.
\\

\textit{Кстати, не всегда берут среднее значение ошибки, то есть не всегда ставят $\frac{1}{l}$}.

\subsection{Сведение задачи обучения к задаче оптимизации}

\textit{Минимизация эмпирического риска} \textbf{empirical risk minimization}:

$$\mu(X^l) = arg\ \underset{a \in A}{min}\ Q(a, X^l)$$

Как только мы сформулировали, что мы считаем ошибки и выписали функционал средней ошибки, мы можем уже решить нашу задачу оптимизации, применяя различные методы. Например, один из самых популярных -- \textit{метод наименьших квадратов} ($Y = \mathbb{R},\ \mathscr{L}$ квадратична):

$$\mu(X^l) = arg\ \underset{\theta}{min}\ \sum\limits_{i = 1}^l (g(x_i, \theta) - y_i)^2$$

Метод удобен тем, что, во-первых, убирает знак, а во-вторых, в отличии от абсолютной разницы с модулем, удобно диффиренцируем по параметрам. 

\subsection{Понятие обобщающей способности (generalization performance)}

Теоретически все выглядит хорошо, но если искать подводные камни, можно задаться следующими вопросами:

\begin{itemize}
\item Найдем ли мы "закон природы" или \textit{переобучимся}, то есть подгоним функцию $g(x_i, \theta)$ под заданные точки?

\item Будет ли $a = \mu(X^l)$ приближать функцию $y$ на всем $X$?

\item Будет ли $Q(a, X^k)$ мал\'o на новых данных ---
контрольной выборке $X^k = (x_i^{'}, y_i^{'})_{i = 1}^k$,\\$y_i^{'} = y(x_i)$?

\end{itemize}

\section{Резюме}

\textbf{Основные понятия машинного обучения:}
\begin{itemize}
\item объект
\item ответ
\item признак
\item предсказательная модель
\item метод обучения
\item эмпирический риск
\item переобучение
\end{itemize}

\end{document}