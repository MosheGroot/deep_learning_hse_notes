\documentclass{article}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage[left=3cm,right=3cm,
    top=3cm,bottom=3cm,bindingoffset=0cm]{geometry}

\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}


\usepackage{setspace}
\usepackage{indentfirst}
\usepackage{textcomp}
\usepackage{ifthen}
\usepackage{calc}
\usepackage{mathrsfs}
\usepackage[dvipsnames]{xcolor}


\title{Введение в машинное обучение}
\author{Национальный исследовательский университет "Высшая школа экономики" \and Yandex School of Data Analysis\\\\
Неофициальный конспект по курсу.}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Метрические методы классификации в задаче восстановления регрессии}
\subsection{Параметрический подход}

Ранее мы рассмотрели \textit{метрические методы классификации} (методы ближайших соседей, окна Парзена и потенциальных функций), основанные на идее измерения \textit{расстояний между объектами}. Эту же идею можно \textit{перенести} и на \textit{задачу восстановления регрессии}.
\\

В задаче регрессии обычно задана обучающая выборка, пара «объект-ответ», в которых ответы --- это действительные числа:

\begin{itemize}
\item $X$ --- объекты (часто $\mathbb{R}^n$);
$Y$ --- ответы (часто $\mathbb{R}$, реже $\mathbb{R}^m$);

$X^l = (x_i, y_i)_{i = 1}^l$ --- обучающая выборка размера $l$;

$y_i = y(x_i)$, $y: X \rightarrow Y$ --- неизвестная зависимость;
\end{itemize}

 И стандартным подходом к решению регрессионных задач является фиксация некоторой параметрической модели зависимости --- функция $f$ от объекта $x$ и вектора \textit{параметров } $\alpha$:

\begin{itemize}
\item $a(x) = f(x, \alpha)$ --- параметрическая модель зависимости,\\
$\alpha \in \mathbb{R}^p$ --- вектор параметров модели.

\end{itemize} 
 И далее идет процесс \textit{определения вектора параметров} с помощью метода наименьших квадратов. Для этого выписывается \textit{функционал} среднего квадрата ошибки и ставится \textit{оптимизационная задача}: найти вектор параметров, доставляющего этому функционалу минимум:
 
\begin{itemize}
\item Метод наименьших квадратов (МНК):

$$Q(\alpha, X^l) = \sum\limits_{i = 1}^l w_i \Bigl(f(x_i, \alpha) - y_i\Bigr)^2
\rightarrow \underset{\alpha}{min},$$

где $w_i$ --- вес, степень важности $i$-го объекта.
\end{itemize}
 
При этом мы в данном случае рассматриваем этот функционал в несколько \textit{обобщенном виде} и ввели веса объектов или \textit{степени важности объектов} обучающей выборки (обычно в функционале такого не делают). 
\\

\begin{itemize}
\item[] \textbf{Недостаток:}\\
надо заранее иметь хорошую параметрическую модель $f(x, \alpha)$.
\end{itemize}

Далеко не всегда в распоряжении исследователей имеются такие модели, поэтому хотелось бы \textit{отходить от параметрического подхода}.

\newpage
\subsection{Непараметрическая регрессия. Формула Надарая-Ватсона}

Идеи \textbf{непараметрических методов} заключаются в том, чтобы приблизить искомую зависимость константы, но локально в окрестности того объекта x, в котором мы хотим вычислить нашу аппроксимирующую функцию:

\begin{center}
\line(1,0){400}
\end{center}

\begin{itemize}
\item[] Приближение константой $f(x, \alpha) = \alpha$ в окрестности $x \in X$:

$$Q(\alpha; X^l) = \sum\limits_{i = 1}^l
\textcolor{red}{w_i(x)}\bigl(\alpha - y_i\bigr)^2
\rightarrow \underset{\alpha \in \mathbb{R}}{min};$$

где $\textcolor{red}{w_i(x)} = K \Bigl(\frac{\rho(x, x_i)}{h}\Bigr)$ --- веса объектов $x_i$ относительно $x$;

$K(r)$ --- ядро, невозрастающее, ограниченное, гладкое (желательно);

$h$ --- ширина окна сглаживания.
\end{itemize}

\begin{center}
\line(1,0){400}
\end{center}

Для этого мы снова пользуемся методом \textit{наименьших квадратов}, а вместо \textit{параметрической модели} зависимости подставляем \textit{константу $\alpha$}, но теперь вся сложность задачи у нас перемещается в \textit{веса объектов}.

Эти \textit{веса} $w_i$ мы задаем в зависимости от того объекта $x$, в котором мы ищем значение аппроксимирующей функции. Соответственно, используем \textit{функцию расстояния} $\rho$ (чтобы вес был тем меньше, чем дальше объект $x$ до объекта обучающей выборки $x_i$), к которой применяем к ней некое \textit{ядро} $K$. Также мы используем ширину окна $h$, чтобы варьировать скорость убывания этой функции по мере возрастания расстояния между объектами.
\\

Нам потребуется выразить $\alpha$. В целом сделать это достаточно просто:  у нас функционал $Q(\alpha)$, нам необходимо найти его минимум, продифференцируем по $\alpha$, приравняем к нулю производную и отсюда найдем $\alpha$ (советуем сделать самостоятельно, как упражнение). Таким образом получаем:

\begin{center}
\line(1,0){400}
\end{center}

\textbf{Формула ядерного сглаживания Надарая-Ватсона:}

$$a_h(x; X^l) = \frac{\sum_{i = 1}^l y_i\textcolor{red}{w_i(x)}}
						{\sum_{i = 1}^l \textcolor{red}{w_i(x)}}
= \frac{\sum_{i = 1}^l y_i K\bigl(\frac{\rho(x, x_i)}{h}\bigr)}
		{\sum_{i = 1}^l K\bigl(\frac{\rho(x, x_i)}{h}\bigr)}$$
		
\begin{center}
\line(1,0){400}
\end{center}

По сути это просто \textit{средневзвешанное значение ответов $y_i$} на объектах обучающей выборки. Веса уже зависят от того, \textit{насколько $i$-й объект далек} от того объекта $x$, в котором мы вычисляем функцию --- \textit{чем дальше, тем меньше вес}.
\end{document}
